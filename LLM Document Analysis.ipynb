{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825ab9e7-3662-4b7f-ab28-29eb8bc3f387",
   "metadata": {},
   "source": [
    "# LLM Document Analysis\n",
    "\n",
    "**Name:** Jose Ayala<br>\n",
    "**Class:** CAP5619 - Artificial Intelligence for FinTech<br>\n",
    "**Assignment:** Text Extraction<br>\n",
    "**Date:** March 11, 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36df3b-ff4c-49fb-ad2a-f1f81315717b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589ea88c-d347-4ff9-aca2-0ee6dcb6093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30cd0f-52a9-4d67-9125-a9368c274b2f",
   "metadata": {},
   "source": [
    "### Fetching Company Data from the SEC API\n",
    "\n",
    "This block of code fetches company data from the SECâ€™s public API, which provides company ticker symbols and related information in JSON format. The JSON data is then parsed and converted into a Pandas DataFrame for further analysis. We'll use this data to extract the necessary company information for processing SEC 8-K filings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6dc7c67-fa44-49a6-8219-5b1ef098da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cik_str ticker                        title\n",
      "9715  1949257  RDPTF  Radiopharm Theranostics Ltd\n",
      "9716  1506721  BLFBY       Balfour Beatty plc/ADR\n",
      "9717  1506721  BAFBF       Balfour Beatty plc/ADR\n",
      "9718  1991946  CGBSW       Crown LNG Holdings Ltd\n",
      "9719  1449664  NWSZF             CTF Services Ltd\n"
     ]
    }
   ],
   "source": [
    "# Fetch the company data from the SEC API\n",
    "url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "headers = {'User-Agent': 'Your Name (jo647937@ucf.edu)'}\n",
    "response = requests.get(url, headers=headers)\n",
    "company_data = response.json()\n",
    "df = pd.DataFrame.from_dict(company_data, orient='index')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961a008-dfab-43f6-be0d-a528261d0f96",
   "metadata": {},
   "source": [
    "### Defining Variables for Company Processing\n",
    "\n",
    "In this block of code, we define some important variables that will be used throughout the process of handling SEC 8-K filings. These variables will guide the flow of data extraction and help manage the processing of a large number of company filings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7e2520-5851-4628-8d1a-7ef7e6031a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of companies to process\n",
    "number_of_companies_to_process = 10000\n",
    "keywords_found = 0\n",
    "final_content_with_keyword = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49cd1d-3b75-420d-9364-070995606368",
   "metadata": {},
   "source": [
    "### Processing SEC 8-K Filings\n",
    "\n",
    "This block of code fetches and processes SEC 8-K filings for a specified number of companies. It starts by retrieving company data and iterates through each company to extract relevant filing information. For each filing, it checks for specific keywords related to new or updated products. The content is cleaned and stored if any keywords are found. The script also tracks the processing time and stops once the desired number of companies has been processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71cb743-bbe0-4d4f-bf66-3c35107599a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URLs - Time elapsed: 25016sThe number of keywords found were  268\n",
      "Number of strings with matching keywords: 268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each of the companies\n",
    "company_count = 0\n",
    "for key, company in company_data.items():\n",
    "\n",
    "    # End processing when reaching the specified number of companies\n",
    "    if company_count >= number_of_companies_to_process:\n",
    "        break\n",
    "\n",
    "    cik = company.get('cik_str')\n",
    "    company_name = company.get('title')\n",
    "    company_ticker = company.get('ticker') \n",
    "\n",
    "    # Construct the 8-K URL for each CIK\n",
    "    url_8k = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=8-K&count=10&output=atom\"\n",
    "    response_8k = requests.get(url_8k, headers=headers)\n",
    "\n",
    "    # Parse the response as XML\n",
    "    soup = BeautifulSoup(response_8k.text, 'xml')\n",
    "\n",
    "    # Find all 8-K entries and extract the url\n",
    "    entries = soup.find_all('entry')\n",
    "    for entry in entries:\n",
    "        filing_href = entry.find('content').find('filing-href').text\n",
    "\n",
    "        # Send a GET request to retrieve the content of the 8-K filing\n",
    "        response_html = requests.get(filing_href, headers=headers)\n",
    "        soup_html = BeautifulSoup(response_html.text, 'html.parser')\n",
    "\n",
    "        # Find all rows in the table and loop through the rows to find the 8-K document\n",
    "        rows = soup_html.find_all('tr')\n",
    "        base_url = \"https://www.sec.gov\"\n",
    "\n",
    "        for row in rows:\n",
    "            description_cell = row.find_all('td')\n",
    "\n",
    "            # Check if the second column is '8-K' (Form Type)\n",
    "            if description_cell and len(description_cell) > 1 and description_cell[1].text.strip() == '8-K':\n",
    "                \n",
    "                # Extract the link from the third column\n",
    "                link_tag = description_cell[2].find('a', href=True)\n",
    "                \n",
    "                if link_tag:\n",
    "                    partial_url = link_tag['href']\n",
    "\n",
    "                    # Remove the '/ix?doc=' part of the url if it exists, if not use as is\n",
    "                    if partial_url.startswith('/ix?doc='):\n",
    "                        document_url = partial_url.replace('/ix?doc=', '')\n",
    "                        full_url = base_url + document_url\n",
    "                    else:\n",
    "                        full_url = base_url + partial_url\n",
    "\n",
    "                    # Send a GET request to retrieve the content of the document\n",
    "                    response = requests.get(full_url, headers=headers)\n",
    "\n",
    "                    # Parse the content\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Extract the 'dei:DocumentPeriodEndDate'\n",
    "                    date_tag = soup.find(attrs={\"name\": \"dei:DocumentPeriodEndDate\"})\n",
    "                    if date_tag:\n",
    "                        document_period_end_date = date_tag.text.strip()\n",
    "\n",
    "                    # Find all occurrences of 'Item' in the text\n",
    "                    item_tags = soup.find_all(string=lambda text: text and text.startswith(\"Item\"))\n",
    "\n",
    "                    # Process the \"Item\" sections\n",
    "                    collecting = False\n",
    "                    final_content = ''\n",
    "                    content_between = []\n",
    "                    \n",
    "                    for item_tag in item_tags:\n",
    "                        # Start collecting when we find \"Item\"\n",
    "                        if \"Item\" in item_tag and not collecting:\n",
    "                            collecting = True\n",
    "                            content_between.append(item_tag.strip())\n",
    "\n",
    "                            # Get the next sibling or the tag that contains the item details\n",
    "                            next_tag = item_tag.find_parent().find_next(\n",
    "                                'div')\n",
    "\n",
    "                            while next_tag:\n",
    "                                # Stop if we reach \"Item 9.01\"  or \"SIGNATURE\" and exclude its content\n",
    "                                if \"Item 9.01\" in next_tag.get_text() or \"SIGNATURE\" in next_tag.get_text():\n",
    "                                    break\n",
    "\n",
    "                                content_between.append(next_tag.get_text(strip=True))\n",
    "                                next_tag = next_tag.find_next('div')\n",
    "\n",
    "                    # Process the extracted content\n",
    "                    for content in content_between:\n",
    "                        cleaned_content = re.sub(r'\\s+', ' ', content)  # Remove extra spaces and line breaks\n",
    "                        cleaned_content = re.sub(r'[^A-Za-z0-9.,;: ]+', '', cleaned_content)  # Remove special characters\n",
    "                        final_content += cleaned_content + \" \"  # Append cleaned content with space separator\n",
    "\n",
    "                    # Remove any trailing spaces and change to all lower case\n",
    "                    final_content = final_content.strip()\n",
    "                    final_content = final_content.lower()\n",
    "                    final_content = f\"Company_Name: {company_name} Ticker: {company_ticker} FilingTime: {document_period_end_date} SEC 8-k: {final_content}\"\n",
    "\n",
    "                    # Keywords that indicate new/updated products\n",
    "                    keywords = [\n",
    "                        \"new product\", \"product launch\", \"introduced a new\", \"debut of\",\n",
    "                        \"unveiled\", \"first-of-its-kind\", \"groundbreaking product\", \"updated product\",\n",
    "                        \"enhanced version\", \"improved\", \"new feature\",\n",
    "                        \"upgrade to\", \"next-generation\", \"redesigned\", \"rebranded\", \"expansion of\",\n",
    "                        \"available for purchase\", \"began shipping\", \"now available\", \"pre-order\",\n",
    "                        \"commercial launch\", \"market release\"\n",
    "                    ]\n",
    "\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in final_content:\n",
    "                            keywords_found += 1\n",
    "                            final_content_with_keyword .append(final_content)\n",
    "                            break\n",
    "\n",
    "    company_count += 1\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_seconds = int(elapsed_time)\n",
    "    \n",
    "    # Print the refreshed time every second\n",
    "    print(f\"\\rProcessing URLs - Time elapsed: {elapsed_seconds}s\", end='', flush=True)\n",
    "\n",
    "print(\"The number of keywords found were \", keywords_found)\n",
    "print(f\"Number of strings with matching keywords: {len(final_content_with_keyword)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3cde89-705b-473b-98b1-c869bdd26e22",
   "metadata": {},
   "source": [
    "### Processing Content with Ollama\n",
    "\n",
    "This code processes extracted content using the Ollama API to analyze SEC 8-K filings for new product information. It sends prompts containing filing details to Ollama and retrieves structured information such as company name, ticker, filing time, new product name, and a brief description. The response is parsed, and the relevant data is extracted using regular expressions. The extracted data is then appended to a CSV file for further analysis, with the company name, ticker, filing time, new product, and description being recorded. If no new product information is found, placeholders (\"none\") are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6462aa7-c490-4a95-a103-4b478285c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing the extracted information using Ollama\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted content using Ollama\n",
    "def run_ollama(question: str) -> str:\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3.2:1b\", question],\n",
    "            capture_output=True, text=True, encoding='utf-8'\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "        \n",
    "\n",
    "for content in final_content_with_keyword:\n",
    "    # Truncate the content\n",
    "    if len(content) > 5000:\n",
    "        content = content[:5000] + \"...\"\n",
    "\n",
    "    # Create the prompt for Ollama\n",
    "    question = f\"\"\"\n",
    "    Review the following SEC 8-k filing and extract the following information about new products.\n",
    "    Information to Extract:\n",
    "    Company Name: Name of the company\n",
    "    Ticker: Ticker of the company\n",
    "    Filing_Time: The document period end date of the 8-k\n",
    "    New Product: The name of the new product\n",
    "    Description: A description of the new product in less than 180 characters.\n",
    "    Here is the 8-k filing - {content}\n",
    "    Remember you must return the company_name, ticker, filing_time, New Product (if any), and Description (if any).\n",
    "    Please respond with all 5 pieces of information.\n",
    "    If no new product information if found, just respond with the company_name, ticker, filing_time, and for new_product and description indicate none.\n",
    "    \"\"\"\n",
    "\n",
    "    ollama_response = run_ollama(question)\n",
    "\n",
    "    # Patterns to match in response\n",
    "    patterns = {\n",
    "        'company_name': r'Company\\s*Name:\\s*(.*)',\n",
    "        'ticker': r'Ticker:\\s*(.*)',\n",
    "        'filing_time': r'Filing\\s*Time:\\s*(.*)',\n",
    "        'new_product': r'New\\s*Product:\\s*(.*)',\n",
    "        'description': r'Description:\\s*(.*)'\n",
    "    }\n",
    "\n",
    "    # Method to Extract information from response\n",
    "    def extract_information(response: str) -> dict:\n",
    "        extracted = {}\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            extracted[key] = match.group(1).strip() if match else 'None'\n",
    "        return extracted\n",
    "\n",
    "    # Method to append information to CSV\n",
    "    def append_to_csv(file_name: str, data: dict):\n",
    "        fieldnames = ['company_name', 'ticker', 'filing_time', 'new_product', 'description']\n",
    "        file_exists = os.path.isfile(file_name)\n",
    "        \n",
    "        with open(file_name, 'a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter=\"|\") \n",
    "            \n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            writer.writerow(data)\n",
    "\n",
    "    # Extract the information and put in CSV\n",
    "    extracted_data = extract_information(ollama_response)\n",
    "    append_to_csv(\"SEC 8-K Analysis\", extracted_data)\n",
    "\n",
    "print(\"Completed processing the extracted information using Ollama\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4d43c-92f3-40de-b495-ec856d9cae25",
   "metadata": {},
   "source": [
    "### Filtering and Saving Processed SEC 8-K Data\n",
    "\n",
    "This code reads a CSV file containing SEC 8-K analysis data and filters out rows where any column contains the value 'none'. It processes the data by checking for 'none' in the `company_name`, `ticker`, `filing_time`, `new_product`, and `description` columns. If any of these fields contains 'none', the row is excluded. The filtered data is then written to a new CSV file called \"SEC 8-K Analysis Filtered\", retaining the same structure with pipe delimiters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1869ffad-e3b5-4e80-aa81-476704d4b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a list of dictionaries\n",
    "with open('SEC 8-K Analysis', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file, delimiter='|')  # Assuming the pipe delimiter\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Filter out rows where any column has 'none'\n",
    "filtered_data = [\n",
    "    row for row in data\n",
    "    if row['company_name'].strip().lower() != 'none' and row['ticker'].strip().lower() != 'none' \n",
    "    and row['filing_time'].strip().lower() != 'none'\n",
    "    and 'none' not in row['new_product'].strip().lower()\n",
    "    and 'none' not in row['description'].strip().lower()\n",
    "]\n",
    "\n",
    "# Define the fieldnames (headers)\n",
    "fieldnames = ['company_name', 'ticker', 'filing_time', 'new_product', 'description']\n",
    "\n",
    "# Write the filtered data to a new CSV file\n",
    "with open('SEC 8-K Analysis Filtered', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames, delimiter='|')\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the filtered rows\n",
    "    for row in filtered_data:\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1e549-bf79-440b-b54b-2c5ecfc8e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
